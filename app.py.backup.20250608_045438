#!/usr/bin/env python3
"""
RAG Document Chat System with Enhanced Processing
A complete retrieval augmented generation system for document Q&A
"""

import os
import asyncio
import sys
import time
import logging
import re
import hashlib
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass

# Core dependencies
import boto3
import chromadb
from openai import OpenAI
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
import PyPDF2
import io

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
class Config:
    """System configuration from environment variables"""
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    AWS_ACCESS_KEY_ID: str = os.getenv("AWS_ACCESS_KEY_ID", "")
    AWS_SECRET_ACCESS_KEY: str = os.getenv("AWS_SECRET_ACCESS_KEY", "")
    AWS_REGION: str = os.getenv("AWS_REGION", "us-east-1")
    S3_BUCKET: str = os.getenv("S3_BUCKET", "")
    CHROMA_HOST: str = os.getenv("CHROMA_HOST", "localhost")
    CHROMA_PORT: int = int(os.getenv("CHROMA_PORT", "8002"))
    
    # Enhanced processing options
    ENABLE_ENHANCED_PROCESSING: bool = os.getenv("ENABLE_ENHANCED_PROCESSING", "true").lower() == "true"
    MAX_CHUNK_SUMMARY_LENGTH: int = int(os.getenv("MAX_CHUNK_SUMMARY_LENGTH", "120"))
    MAX_KEY_TERMS: int = int(os.getenv("MAX_KEY_TERMS", "5"))
    
    @property
    def s3_enabled(self) -> bool:
        return bool(self.S3_BUCKET and self.AWS_ACCESS_KEY_ID and self.AWS_SECRET_ACCESS_KEY)
    
    @property
    def openai_enabled(self) -> bool:
        return bool(self.OPENAI_API_KEY and self.OPENAI_API_KEY.startswith("sk-"))

config = Config()

# Pydantic models
class ChatRequest(BaseModel):
    query: str
    top_k: int = 3

class ChatResponse(BaseModel):
    answer: str
    sources: List[str]
    processing_time: float

class DocumentResponse(BaseModel):
    status: str
    message: str
    chunks_created: int = 0
    processing_time: float = 0.0

# Enhanced metadata dataclass
@dataclass
class ChunkMetadata:
    """Enhanced metadata for document chunks"""
    filename: str
    chunk_index: int
    total_chunks: int
    chunk_size: int
    chunk_summary: str
    page_number: Optional[int]
    section_title: Optional[str]
    start_char: int
    end_char: int
    paragraph_number: int
    content_type: str
    key_terms: List[str]
    chunk_hash: str

class EnhancedProcessor:
    """Enhanced document processor with better chunk metadata"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    def extract_page_numbers(self, text: str) -> Dict[int, int]:
        """Extract page numbers and their positions in text"""
        page_positions = {}
        page_patterns = [
            r'Page\s+(\d+)',
            r'page\s+(\d+)', 
            r'\[Page\s+(\d+)\]',
            r'(?:^|\n)\s*(\d+)\s*(?:\n|$)'
        ]
        
        for pattern in page_patterns:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                page_num = int(match.group(1))
                position = match.start()
                page_positions[position] = page_num
        
        return page_positions
    
    def extract_section_titles(self, text: str) -> Dict[int, str]:
        """Extract section titles and their positions"""
        section_positions = {}
        patterns = [
            r'^#{1,6}\s+(.+)$',  # Markdown headers
            r'^([A-Z][A-Z\s]{2,})$',  # ALL CAPS headers
            r'^\d+\.\s+(.+)$',  # Numbered sections
            r'^Chapter\s+\d+:?\s*(.*)$',  # Chapter headers
        ]
        
        lines = text.split('\n')
        char_position = 0
        
        for line in lines:
            line_stripped = line.strip()
            for pattern in patterns:
                match = re.match(pattern, line_stripped, re.IGNORECASE)
                if match:
                    title = match.group(1).strip()
                    if len(title) > 3:
                        section_positions[char_position] = title
                    break
            char_position += len(line) + 1
        
        return section_positions
    
    def get_page_number_for_position(self, position: int, page_positions: Dict[int, int]) -> Optional[int]:
        """Get page number for a given character position"""
        if not page_positions:
            return None
        relevant_pages = [(pos, page) for pos, page in page_positions.items() if pos <= position]
        if relevant_pages:
            return max(relevant_pages, key=lambda x: x[0])[1]
        return None
    
    def get_section_title_for_position(self, position: int, section_positions: Dict[int, str]) -> Optional[str]:
        """Get section title for a given character position"""
        if not section_positions:
            return None
        relevant_sections = [(pos, title) for pos, title in section_positions.items() if pos <= position]
        if relevant_sections:
            return max(relevant_sections, key=lambda x: x[0])[1]
        return None
    
    def generate_chunk_summary(self, chunk_text: str) -> str:
        """Generate a concise summary of the chunk content"""
        clean_text = re.sub(r'\s+', ' ', chunk_text.strip())
        sentences = [s.strip() for s in clean_text.split('.') if len(s.strip()) > 10]
        
        if sentences:
            summary = sentences[0]
            if len(summary) < 50 and len(sentences) > 1:
                summary += ". " + sentences[1]
        else:
            summary = clean_text[:config.MAX_CHUNK_SUMMARY_LENGTH]
        
        if len(summary) > config.MAX_CHUNK_SUMMARY_LENGTH:
            summary = summary[:config.MAX_CHUNK_SUMMARY_LENGTH-3] + "..."
        
        return summary
    
    def extract_key_terms(self, chunk_text: str) -> List[str]:
        """Extract key terms from chunk text"""
        text = re.sub(r'[^\w\s]', ' ', chunk_text)
        words = text.split()
        
        capitalized = [w for w in words if w[0].isupper() and len(w) > 2]
        long_words = [w.lower() for w in words if len(w) > 6]
        
        key_terms = list(set(capitalized + long_words))
        term_freq = {term: chunk_text.lower().count(term.lower()) for term in key_terms}
        sorted_terms = sorted(term_freq.items(), key=lambda x: x[1], reverse=True)
        
        return [term for term, freq in sorted_terms[:config.MAX_KEY_TERMS]]
    
    def determine_content_type(self, chunk_text: str) -> str:
        """Determine the type of content in the chunk"""
        text_lower = chunk_text.lower()
        
        if re.search(r'\d+\.\s+.*\d+\.\s+.*\d+\.\s+', chunk_text):
            return "numbered_list"
        elif re.search(r'[‚Ä¢\-\*]\s+.*[‚Ä¢\-\*]\s+', chunk_text):
            return "bullet_list"
        elif re.search(r'(table|column|row)', text_lower):
            return "table_content"
        elif re.search(r'(figure|chart|graph|image)', text_lower):
            return "figure_reference"
        elif re.search(r'(introduction|overview|summary|conclusion)', text_lower):
            return "summary_content"
        elif re.search(r'(step|procedure|method|process)', text_lower):
            return "procedural"
        else:
            return "general_text"
    
    def calculate_chunk_hash(self, chunk_text: str) -> str:
        """Calculate hash for chunk deduplication"""
        return hashlib.md5(chunk_text.encode()).hexdigest()[:12]
    
    def count_paragraphs_before_position(self, text: str, position: int) -> int:
        """Count paragraphs before given position"""
        text_before = text[:position]
        return text_before.count('\n\n') + 1

class RAGSystem:
    """Main RAG system implementation with enhanced processing"""
    
    def __init__(self):
        logger.info("Initializing RAG System...")
        
        # Initialize S3 client if enabled
        self.s3_client = None
        if config.s3_enabled:
            try:
                self.s3_client = boto3.client(
                    's3',
                    aws_access_key_id=config.AWS_ACCESS_KEY_ID,
                    aws_secret_access_key=config.AWS_SECRET_ACCESS_KEY,
                    region_name=config.AWS_REGION
                )
                logger.info("‚úÖ S3 client initialized")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è S3 initialization failed: {e}")
                self.s3_client = None
        
        # Initialize ChromaDB
        self.chroma_client = None
        self.collection = None
        self._init_chromadb()
        
        # Initialize text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        # Initialize enhanced processor
        self.enhanced_processor = EnhancedProcessor()
        
        # Initialize OpenAI client
        if config.openai_enabled:
            try:
                self.openai_client = OpenAI(api_key=config.OPENAI_API_KEY)
                self.openai_client.models.list()
                logger.info("‚úÖ OpenAI client initialized")
            except Exception as e:
                logger.error(f"‚ùå OpenAI initialization failed: {e}")
                raise ValueError("OpenAI API key is required.")
        else:
            raise ValueError("OpenAI API key is required.")

    def _init_chromadb(self) -> None:
        """Initialize ChromaDB with connection retries"""
        for attempt in range(3):
            try:
                logger.info(f"üîÑ Connecting to ChromaDB (attempt {attempt + 1}/3)...")
                
                self.chroma_client = chromadb.HttpClient(
                    host=config.CHROMA_HOST, 
                    port=config.CHROMA_PORT
                )
                
                self.chroma_client.heartbeat()
                
                self.collection = self.chroma_client.get_or_create_collection(
                    name="documents",
                    metadata={"description": "RAG document collection"}
                )
                logger.info("‚úÖ ChromaDB server connected")
                return
                
            except Exception as e:
                logger.warning(f"‚ùå ChromaDB connection attempt {attempt + 1} failed: {e}")
                if attempt < 2:
                    time.sleep(2)
        
        # Fallback to in-memory client
        logger.info("‚ö†Ô∏è Using in-memory ChromaDB (data will not persist)")
        self.chroma_client = chromadb.Client()
        self.collection = self.chroma_client.get_or_create_collection("documents")

    def extract_text(self, file_content: bytes, filename: str) -> str:
        """Extract text from uploaded files"""
        file_ext = Path(filename).suffix.lower()
        
        try:
            if file_ext == '.pdf':
                reader = PyPDF2.PdfReader(io.BytesIO(file_content))
                text = ""
                for page_num, page in enumerate(reader.pages):
                    try:
                        text += page.extract_text() + "\n"
                    except Exception as e:
                        logger.warning(f"Failed to extract text from page {page_num}: {e}")
                return text.strip()
            
            elif file_ext == '.txt':
                try:
                    return file_content.decode('utf-8')
                except UnicodeDecodeError:
                    return file_content.decode('utf-8', errors='ignore')
            
            else:
                raise ValueError(f"Unsupported file type: {file_ext}")
                
        except Exception as e:
            logger.error(f"Text extraction failed for {filename}: {e}")
            raise
    
    def get_embedding(self, text: str) -> List[float]:
        """Generate embedding for text using OpenAI"""
        try:
            response = self.openai_client.embeddings.create(
                model="text-embedding-ada-002",
                input=text[:8191]
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            raise
    
    def process_document_with_enhanced_metadata(self, text: str, filename: str) -> List[Tuple[str, ChunkMetadata]]:
        """Process document and create enhanced metadata for each chunk"""
        
        # Pre-analyze document structure
        page_positions = self.enhanced_processor.extract_page_numbers(text)
        section_positions = self.enhanced_processor.extract_section_titles(text)
        
        # Split into chunks with position tracking
        chunks = []
        current_position = 0
        
        text_chunks = self.enhanced_processor.text_splitter.split_text(text)
        
        for i, chunk_text in enumerate(text_chunks):
            # Find this chunk's position in original text
            chunk_start = text.find(chunk_text, current_position)
            if chunk_start == -1:
                chunk_start = current_position
            
            chunk_end = chunk_start + len(chunk_text)
            current_position = chunk_end
            
            # Generate enhanced metadata
            metadata = ChunkMetadata(
                filename=filename,
                chunk_index=i,
                total_chunks=len(text_chunks),
                chunk_size=len(chunk_text),
                chunk_summary=self.enhanced_processor.generate_chunk_summary(chunk_text),
                page_number=self.enhanced_processor.get_page_number_for_position(chunk_start, page_positions),
                section_title=self.enhanced_processor.get_section_title_for_position(chunk_start, section_positions),
                start_char=chunk_start,
                end_char=chunk_end,
                paragraph_number=self.enhanced_processor.count_paragraphs_before_position(text, chunk_start),
                content_type=self.enhanced_processor.determine_content_type(chunk_text),
                key_terms=self.enhanced_processor.extract_key_terms(chunk_text),
                chunk_hash=self.enhanced_processor.calculate_chunk_hash(chunk_text)
            )
            
            chunks.append((chunk_text, metadata))
        
        return chunks
    
    def create_searchable_metadata_dict(self, metadata: ChunkMetadata) -> Dict:
        """Convert metadata to dictionary for ChromaDB storage"""
        return {
            "filename": metadata.filename,
            "chunk_index": metadata.chunk_index,
            "total_chunks": metadata.total_chunks,
            "chunk_size": metadata.chunk_size,
            "chunk_summary": metadata.chunk_summary,
            "page_number": metadata.page_number,
            "section_title": metadata.section_title or "Unknown Section",
            "start_char": metadata.start_char,
            "end_char": metadata.end_char,
            "paragraph_number": metadata.paragraph_number,
            "content_type": metadata.content_type,
            "key_terms": ", ".join(metadata.key_terms),
            "chunk_hash": metadata.chunk_hash,
            "location_reference": f"Page {metadata.page_number or 'N/A'}, Section: {metadata.section_title or 'Unknown'}, Paragraph {metadata.paragraph_number}"
        }
    
    async def process_document(self, file_content: bytes, filename: str) -> DocumentResponse:
        """Process uploaded document: extract text, chunk, embed, and store"""
        start_time = time.time()
        
        try:
            logger.info(f"üìÑ Processing document: {filename}")
            
            # Extract text
            text = self.extract_text(file_content, filename)
            if not text.strip():
                return DocumentResponse(
                    status="error",
                    message="No text content found in document",
                    processing_time=time.time() - start_time
                )
            
            logger.info(f"üìù Extracted {len(text)} characters")
            
            # Upload to S3 if configured
            if self.s3_client:
                try:
                    self.s3_client.put_object(
                        Bucket=config.S3_BUCKET,
                        Key=f"documents/{filename}",
                        Body=file_content,
                        Metadata={'original_name': filename}
                    )
                    logger.info(f"‚òÅÔ∏è Uploaded to S3: {filename}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è S3 upload failed: {e}")
            
            # Choose processing method based on configuration
            if config.ENABLE_ENHANCED_PROCESSING:
                chunks_with_metadata = self.process_document_with_enhanced_metadata(text, filename)
                
                if not chunks_with_metadata:
                    return DocumentResponse(
                        status="error",
                        message="Failed to create text chunks",
                        processing_time=time.time() - start_time
                    )
                
                logger.info(f"‚úÇÔ∏è Created {len(chunks_with_metadata)} enhanced chunks")
                
                # Store chunks with enhanced metadata
                for chunk_text, metadata in chunks_with_metadata:
                    try:
                        embedding = self.get_embedding(chunk_text)
                        chunk_id = f"{filename}_{metadata.chunk_index}_{metadata.chunk_hash}"
                        
                        self.collection.add(
                            ids=[chunk_id],
                            embeddings=[embedding],
                            documents=[chunk_text],
                            metadatas=[self.create_searchable_metadata_dict(metadata)]
                        )
                    except Exception as e:
                        logger.error(f"Failed to process chunk {metadata.chunk_index}: {e}")
                        continue
                
                chunks_created = len(chunks_with_metadata)
            else:
                # Basic processing (original method)
                chunks = self.text_splitter.split_text(text)
                if not chunks:
                    return DocumentResponse(
                        status="error",
                        message="Failed to create text chunks",
                        processing_time=time.time() - start_time
                    )
                
                logger.info(f"‚úÇÔ∏è Created {len(chunks)} basic chunks")
                
                for i, chunk in enumerate(chunks):
                    try:
                        embedding = self.get_embedding(chunk)
                        chunk_id = f"{filename}_{i}"
                        
                        self.collection.add(
                            ids=[chunk_id],
                            embeddings=[embedding],
                            documents=[chunk],
                            metadatas=[{
                                "filename": filename,
                                "chunk_index": i,
                                "total_chunks": len(chunks),
                                "chunk_size": len(chunk)
                            }]
                        )
                    except Exception as e:
                        logger.error(f"Failed to process chunk {i}: {e}")
                        continue
                
                chunks_created = len(chunks)
            
            processing_time = time.time() - start_time
            processing_type = "Enhanced" if config.ENABLE_ENHANCED_PROCESSING else "Basic"
            logger.info(f"‚úÖ {processing_type} processing completed for {filename} in {processing_time:.2f}s")
            
            return DocumentResponse(
                status="success",
                message=f"Successfully processed {chunks_created} chunks with {processing_type.lower()} processing",
                chunks_created=chunks_created,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Document processing failed: {e}")
            return DocumentResponse(
                status="error",
                message=f"Processing failed: {str(e)}",
                processing_time=time.time() - start_time
            )
    
    def search_and_answer(self, query: str, top_k: int = 3) -> ChatResponse:
        """Search documents and generate answer using RAG"""
        start_time = time.time()
        
        try:
            logger.info(f"üîç Processing query: {query}")
            
            # Generate query embedding
            query_embedding = self.get_embedding(query)
            
            # Search for relevant chunks
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k
            )
            
            if not results['documents'][0]:
                return ChatResponse(
                    answer="No relevant documents found. Please upload some documents first.",
                    sources=[],
                    processing_time=time.time() - start_time
                )
            
            # Check if we have enhanced metadata
            has_enhanced_metadata = any(
                'location_reference' in meta for meta in results['metadatas'][0]
            )
            
            if has_enhanced_metadata and config.ENABLE_ENHANCED_PROCESSING:
                # Enhanced response with location information
                context_chunks = []
                source_info = []
                
                for chunk, metadata in zip(results['documents'][0], results['metadatas'][0]):
                    location_ref = metadata.get('location_reference', 'Unknown location')
                    summary = metadata.get('chunk_summary', 'No summary available')
                    
                    enhanced_chunk = f"[Source: {location_ref}]\n{chunk}\n[Summary: {summary}]\n"
                    context_chunks.append(enhanced_chunk)
                    
                    source_detail = f"{metadata['filename']} ({location_ref})"
                    source_info.append(source_detail)
                
                context = "\n---\n".join(context_chunks)
                
                logger.info(f"üìö Found {len(context_chunks)} relevant chunks with enhanced metadata")
                
                # Generate answer with location awareness
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {
                            "role": "system", 
                            "content": "You are a helpful assistant that answers questions based on provided context. "
                                     "Each source includes location information in brackets. "
                                     "When referencing information, mention the specific location (page, section) when available. "
                                     "If the context doesn't contain enough information to answer the question, "
                                     "say so clearly. Always be accurate and cite the information from the context."
                        },
                        {
                            "role": "user", 
                            "content": f"Context with location information:\n{context}\n\nQuestion: {query}\n\n"
                                     "Answer the question and reference specific locations when mentioning information:"
                        }
                    ],
                    temperature=0.1,
                    max_tokens=1000
                )
                
                sources = source_info
                
            else:
                # Basic response (original method)
                context_chunks = results['documents'][0]
                context = "\n\n".join(context_chunks)
                sources = [meta["filename"] for meta in results['metadatas'][0]]
                
                logger.info(f"üìö Found {len(context_chunks)} relevant chunks from {len(set(sources))} documents")
                
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {
                            "role": "system", 
                            "content": "You are a helpful assistant that answers questions based on provided context. "
                                     "If the context doesn't contain enough information to answer the question, "
                                     "say so clearly. Always be accurate and cite the information from the context."
                        },
                        {
                            "role": "user", 
                            "content": f"Context:\n{context}\n\nQuestion: {query}\n\nAnswer:"
                        }
                    ],
                    temperature=0.1,
                    max_tokens=1000
                )
                
                sources = list(set(sources))  # Remove duplicates
            
            answer = response.choices[0].message.content
            processing_time = time.time() - start_time
            
            logger.info(f"üí¨ Generated answer in {processing_time:.2f}s")
            
            return ChatResponse(
                answer=answer,
                sources=sources,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Search and answer failed: {e}")
            return ChatResponse(
                answer=f"Sorry, I encountered an error: {str(e)}",
                sources=[],
                processing_time=time.time() - start_time
            )
    
    def get_system_status(self) -> Dict:
        """Get system component status"""
        status = {
            "chromadb": "disconnected",
            "openai": "disconnected",
            "s3": "disabled",
            "enhanced_processing": config.ENABLE_ENHANCED_PROCESSING
        }
        
        # Check ChromaDB
        try:
            if hasattr(self.chroma_client, 'heartbeat'):
                self.chroma_client.heartbeat()
                status["chromadb"] = "connected"
            else:
                status["chromadb"] = "in-memory"
        except:
            status["chromadb"] = "disconnected"
        
        # Check OpenAI
        if config.openai_enabled:
            try:
                self.openai_client.models.list()
                status["openai"] = "connected"
            except:
                status["openai"] = "error"
        
        # Check S3
        if config.s3_enabled and self.s3_client:
            try:
                self.s3_client.head_bucket(Bucket=config.S3_BUCKET)
                status["s3"] = "connected"
            except:
                status["s3"] = "error"
        
        return status

# Initialize RAG system
rag_system = RAGSystem()

# FastAPI Application
app = FastAPI(
    title="RAG Document Chat API",
    description="Retrieval Augmented Generation system for document Q&A with enhanced processing",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "message": "RAG Document Chat API is running!",
        "status": rag_system.get_system_status()
    }

@app.post("/upload", response_model=DocumentResponse)
async def upload_document(file: UploadFile = File(...)):
    """Upload and process a document"""
    try:
        # Validate file
        if not file.filename:
            raise HTTPException(status_code=400, detail="No filename provided")
        
        if not file.filename.lower().endswith(('.pdf', '.txt')):
            raise HTTPException(status_code=400, detail="Only PDF and TXT files are supported")
        
        # Read file content
        content = await file.read()
        if len(content) == 0:
            raise HTTPException(status_code=400, detail="Empty file")
        
        # Process document
        result = await rag_system.process_document(content, file.filename)
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload endpoint error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/chat", response_model=ChatResponse)
async def chat_with_documents(request: ChatRequest):
    """Ask questions about uploaded documents"""
    try:
        if not request.query.strip():
            raise HTTPException(status_code=400, detail="Query cannot be empty")
        
        result = rag_system.search_and_answer(request.query, request.top_k)
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat endpoint error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/status")
async def get_status():
    """Get system status"""
    return rag_system.get_system_status()

# Streamlit Interface
def create_streamlit_app():
    """Create enhanced Streamlit web interface"""
    
    st.set_page_config(
        page_title="RAG Document Chat",
        page_icon="üìö",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üìö RAG Document Chat System")
    if config.ENABLE_ENHANCED_PROCESSING:
        st.markdown("Upload documents and chat with them using AI! ‚ú® **Enhanced processing enabled**")
    else:
        st.markdown("Upload documents and chat with them using AI!")
    
    # Sidebar for system status and file upload
    with st.sidebar:
        st.header("üìä System Status")
        
        status = rag_system.get_system_status()
        
        # Status indicators
        chromadb_status = "üü¢" if status["chromadb"] == "connected" else "üü°" if status["chromadb"] == "in-memory" else "üî¥"
        openai_status = "üü¢" if status["openai"] == "connected" else "üî¥"
        s3_status = "üü¢" if status["s3"] == "connected" else "üü°" if status["s3"] == "disabled" else "üî¥"
        enhanced_status = "üü¢" if status["enhanced_processing"] else "üü°"
        
        st.markdown(f"**ChromaDB:** {chromadb_status} {status['chromadb']}")
        st.markdown(f"**OpenAI:** {openai_status} {status['openai']}")
        st.markdown(f"**S3:** {s3_status} {status['s3']}")
        st.markdown(f"**Enhanced Processing:** {enhanced_status} {'enabled' if status['enhanced_processing'] else 'disabled'}")
        
        st.divider()
        
        # File upload section
        st.header("üìÅ Upload Document")
        uploaded_file = st.file_uploader(
            "Choose a document",
            type=['pdf', 'txt'],
            help="Upload PDF or TXT files to add to your knowledge base"
        )
        
        if uploaded_file is not None:
            if st.button("üîÑ Process Document", use_container_width=True):
                with st.spinner("Processing document..."):
                    try:
                        # Process the document
                        result = asyncio.run(rag_system.process_document(
                            uploaded_file.read(), uploaded_file.name
                        ))
                        
                        if result.status == "success":
                            st.success(f"‚úÖ {result.message}")
                            st.info(f"‚è±Ô∏è Processed in {result.processing_time:.2f}s")
                            
                            if config.ENABLE_ENHANCED_PROCESSING:
                                st.info("üìç Enhanced metadata includes: summaries, locations, page numbers, and key terms")
                        else:
                            st.error(f"‚ùå {result.message}")
                            
                    except Exception as e:
                        st.error(f"‚ùå Error: {str(e)}")
    
    # Main chat interface
    st.header("üí¨ Chat with Your Documents")
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Display chat history
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            # Show enhanced sources for assistant messages
            if message["role"] == "assistant" and "sources" in message and message["sources"]:
                with st.expander("üìö Sources & Locations", expanded=False):
                    for i, source in enumerate(message["sources"], 1):
                        if config.ENABLE_ENHANCED_PROCESSING and " (" in source:
                            # Enhanced source with location
                            filename, location = source.split(" (", 1)
                            location = location.rstrip(")")
                            st.markdown(f"**{i}. {filename}**")
                            st.caption(f"üìç {location}")
                        else:
                            # Basic source
                            st.text(f"‚Ä¢ {source}")
    
    # Chat input
    if prompt := st.chat_input("Ask a question about your documents..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate assistant response
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                try:
                    response = rag_system.search_and_answer(prompt)
                    
                    # Display answer
                    st.markdown(response.answer)
                    
                    # Add to chat history
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": response.answer,
                        "sources": response.sources
                    })
                    
                    # Show sources
                    if response.sources:
                        with st.expander("üìö Sources & Locations", expanded=False):
                            for i, source in enumerate(response.sources, 1):
                                if config.ENABLE_ENHANCED_PROCESSING and " (" in source:
                                    # Enhanced source with location
                                    filename, location = source.split(" (", 1)
                                    location = location.rstrip(")")
                                    st.markdown(f"**{i}. {filename}**")
                                    st.caption(f"üìç {location}")
                                else:
                                    # Basic source
                                    st.text(f"‚Ä¢ {source}")
                    
                    # Show processing time
                    st.caption(f"‚è±Ô∏è Response generated in {response.processing_time:.2f}s")
                    
                except Exception as e:
                    error_msg = f"Sorry, I encountered an error: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg,
                        "sources": []
                    })

def main():
    """Main application entry point"""
    if len(sys.argv) > 1:
        if sys.argv[1] == "streamlit":
            # Run Streamlit interface
            create_streamlit_app()
        elif sys.argv[1] == "api":
            # Run FastAPI server
            import uvicorn
            logger.info("üöÄ Starting FastAPI server...")
            uvicorn.run(app, host="0.0.0.0", port=8001)
        else:
            print("Usage: python app.py [streamlit|api]")
            print("  streamlit - Run web interface (default)")
            print("  api      - Run API server")
            sys.exit(1)
    else:
        # Default to Streamlit
        create_streamlit_app()

if __name__ == "__main__":
    main()
